{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antwon/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/antwon/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import requests\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "class Article:\n",
    "    def __init__(self, article) -> None:\n",
    "        self.revealed_dict = self.build_revealed()\n",
    "        self.tokens = self.tokenize(article, keep_space=True)\n",
    "        self.set_ = set([token.casefold() for token in self.tokens])\n",
    "\n",
    "    def build_revealed(self):\n",
    "        revealed_dict = set()\n",
    "        for word in stopwords:\n",
    "            revealed_dict.add(word)\n",
    "        return revealed_dict\n",
    "    \n",
    "    def add_revealed(self, word):\n",
    "        self.revealed_dict.add(word.casefold())\n",
    "        self.revealed_dict.add(word.capitalize())\n",
    "\n",
    "    def tokenize(self, article, keep_space=True):\n",
    "        if keep_space:\n",
    "            split = re.split('(\\W+)', article)\n",
    "        else:\n",
    "            split = re.split('\\W+',article)\n",
    "        return split\n",
    "    \n",
    "    def sent_tokenize(self, article):\n",
    "        return re.split('[\\.\\n]', article)\n",
    "\n",
    "    def process_article(self, article):\n",
    "        mask = re.sub('█+','[MASK]',article)\n",
    "        return mask\n",
    "\n",
    "    def redact_article(self,bert_mask = False):\n",
    "        redacted = []\n",
    "        for t in self.tokens:\n",
    "            #print(t)\n",
    "            if re.search('\\W+',t):\n",
    "                redacted.append(t)\n",
    "            elif t in self.revealed_dict:\n",
    "                redacted.append(t)\n",
    "            else:\n",
    "                if bert_mask:\n",
    "                    redacted.append('[MASK]')\n",
    "                else:\n",
    "                    redacted.append('█' * len(t))\n",
    "        return redacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = '''\n",
    "Architecture (from Latin  architectura; from Ancient Greek  ἀρχιτέκτων (arkhitéktōn) 'architect'; from  ἀρχι- (arkhi-) 'chief', and  τέκτων (téktōn) 'creator') is the art and technique of designing and building, as distinguished from the skills associated with construction. It is both the process and the product of sketching, conceiving, planning, designing, and constructing buildings or other structures. Architectural works, in the material form of buildings, are often perceived as cultural symbols and as works of art. Historical civilizations are often identified with their surviving architectural achievements.The practice, which began in the prehistoric era, has been used as a way of expressing culture for civilizations on all seven continents. For this reason, architecture is considered to be a form of art. Texts on architecture have been written since ancient times. The earliest surviving text on architectural theories is the 1st century AD treatise De architectura by the Roman architect Vitruvius, according to whom a good building embodies firmitas, utilitas, and venustas (durability, utility, and beauty). Centuries later, Leon Battista Alberti developed his ideas further, seeing beauty as an objective quality of buildings to be found in their proportions. Giorgio Vasari wrote Lives of the Most Excellent Painters, Sculptors, and Architects and put forward the idea of style in the Western arts in the 16th century. In the 19th century, Louis Sullivan declared that \"form follows function\". \"Function\" began to replace the classical \"utility\" and was understood to include not only practical but also aesthetic, psychological and cultural dimensions. The idea of sustainable architecture was introduced in the late 20th century.\n",
    "Architecture began as rural, oral vernacular architecture that developed from trial and error to successful replication. Ancient urban architecture was preoccupied with building religious structures and buildings symbolizing the political power of rulers until Greek and Roman architecture shifted focus to civic virtues. Indian and Chinese architecture influenced forms all over Asia and Buddhist architecture in particular took diverse local flavors. In fact, During the European Middle Ages, pan-European styles of Romanesque and Gothic cathedrals and abbeys emerged while the Renaissance favored Classical forms implemented by architects known by name. Later, the roles of architects and engineers became separated. Modern architecture began after World War I as an avant-garde movement that sought to develop a completely new style appropriate for a new post-war social and economic order focused on meeting the needs of the middle and working classes. Emphasis was put on modern techniques, materials, and simplified geometric forms, paving the way for high-rise superstructures. Many architects became disillusioned with modernism which they perceived as ahistorical and anti-aesthetic, and postmodern and contemporary architecture developed.\n",
    "Over the years, the field of architectural construction has branched out to include everything from ship design to interior decorating.\n",
    "\n",
    "\n",
    "== Definitions ==\n",
    "Architecture can mean:\n",
    "\n",
    "A general term to describe buildings and other physical structures.\n",
    "The art and science of designing buildings and (some) nonbuilding structures.\n",
    "The style of design and method of construction of buildings and other physical structures.\n",
    "A unifying or coherent form or structure.\n",
    "Knowledge of art, science, technology, and humanity.\n",
    "The design activity of the architect, from the macro-level (urban design, landscape architecture) to the micro-level (construction details and furniture). The practice of the architect, where architecture means offering or rendering professional services in connection with the design and construction of buildings, or built environments.\n",
    "\n",
    "\n",
    "== Theory of architecture ==\n",
    "\n",
    "The philosophy of architecture is a branch of philosophy of art, dealing with aesthetic value of architecture, its semantics and in relation with development of culture. Many philosophers and theoreticians from Plato to Michel Foucault, Gilles Deleuze, Robert Venturi and Ludwig Wittgenstein have concerned themselves with the nature of architecture and whether or not architecture is distinguished from building.\n",
    "\n",
    "\n",
    "=== Historic treatises ===\n",
    "The earliest surviving written work on the subject of architecture is De architectura by the Roman architect Vitruvius in the early 1st century AD. According to Vitruvius, a good building should satisfy the three principles of firmitas, utilitas, venustas, commonly known by the original translation – firmness, commodity and delight. An equivalent in modern English would be:\n",
    "\n",
    "Durability – a building should stand up robustly and remain in good condition\n",
    "Utility – it should be suitable for the purposes for which it is used\n",
    "Beauty – it should be aesthetically pleasingAccording to Vitruvius, the architect should strive to fulfill each of these three attributes as well as possible. Leon Battista Alberti, who elaborates on the ideas of Vitruvius in his treatise, De re aedificatoria, saw beauty primarily as a matter of proportion, although ornament also played a part. For Alberti, the rules of proportion were those that governed the idealized human figure, the Golden mean. The most important aspect of beauty was, therefore, an inherent part of an object, rather than something applied superficially, and was based on universal, recognizable truths. The notion of style in the arts was not developed until the 16th century, with the writing of Giorgio Vasari. By the 18th century, his Lives of the Most Excellent Painters, Sculptors, and Architects had been translated into Italian, French, Spanish, and English.\n",
    "In the 16th century, Italian Mannerist architect, painter and theorist Sebastiano Serlio wrote Tutte L'Opere D'Architettura et Prospetiva (Complete Works on Architecture and Perspective). This treatise exerted immense influence throughout Europe, being the first handbook that emphasized the practical rather than the theoretical aspects of architecture, and it was the first to catalog the five orders.In the early 19th century, Augustus Welby Northmore Pugin wrote Contrasts (1836) that, as the title suggested, contrasted the modern, industrial world, which he disparaged, with an idealized image of neo-medieval world. Gothic architecture, Pugin believed, was the only \"true Christian form of architecture.\" The 19th-century English art critic, John Ruskin, in his Seven Lamps of Architecture, published 1849, was much narrower in his view of what constituted architecture. Architecture was the \"art which so disposes and adorns the edifices raised by men … that the sight of them\" contributes \"to his mental health, power, and pleasure\". For Ruskin, the aesthetic was of overriding significance. His work goes on to state that a building is not truly a work of architecture unless it is in some way \"adorned\". For Ruskin, a well-constructed, well-proportioned, functional building needed string courses or rustication, at the very least.On the difference between the ideals of architecture and mere construction, the renowned 20th-century architect Le Corbusier wrote: \"You employ stone, wood, and concrete, and with these materials you build houses and palaces: that is construction. Ingenuity is at work. But suddenly you touch my heart, you do me good. I am happy and I say: This is beautiful. That is Architecture\". Le Corbusier's contemporary Ludwig Mies van der Rohe said \"Architecture starts when you carefully put two bricks together. There it begins.\"\n",
    "\n",
    "\n",
    "=== Modern concepts ===\n",
    "The notable 19th-century architect of skyscrapers, Louis Sullivan, promoted an overriding precept to architectural design: \"Form follows function\". While the notion that structural and aesthetic considerations should be entirely subject to functionality was met with both popularity and skepticism, it had the effect of introducing the concept of \"function\" in place of Vitruvius' \"utility\". \"Function\" came to be seen as encompassing all criteria of the use, perception and enjoyment of a building, not only practical but also aesthetic, psychological and cultural.\n",
    "Nunzia Rondanini stated, \"Through its aesthetic dimension architecture goes beyond the functional aspects that it has in common with other human sciences. Through its own particular way of expressing values, architecture can stimulate and influence social life without presuming that, in and of itself, it will promote social development.... To restrict the meaning of (architectural) formalism to art for art's sake is not only reactionary; it can also be a purposeless quest for perfection or originality which degrades form into a mere instrumentality\".Among the philosophies that have influenced modern architects and their approach to building design are Rationalism, Empiricism, Structuralism, Poststructuralism, Deconstruction and Phenomenology.\n",
    "In the late 20th century a new concept was added to those included in the compass of both structure and function, the consideration of sustainability, hence sustainable architecture. To satisfy the contemporary ethos a building should be constructed in a manner which is environmentally friendly in terms of the production of its materials, its impact upon the natural and built environment of its surrounding area and the demands that it makes upon non-sustainable power sources for heating, cooling, water and waste management, and lighting.\n",
    "\n",
    "\n",
    "\n",
    "\t\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self):\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "        self.article: Article = None\n",
    "        self.Y = None\n",
    "        self.Yset = set()\n",
    "        self.history = {}\n",
    "    \n",
    "    def setArticle(self, article):\n",
    "        self.article = Article(article)\n",
    "    \n",
    "    def setY(self, y: str):\n",
    "        self.Y = y\n",
    "        split = y.split()\n",
    "        self.Yset = set()\n",
    "        for word in split:\n",
    "            self.Yset.add(word.casefold())\n",
    "    \n",
    "    def best_guess_sentence(self, sentence):\n",
    "        bert_token_input = self.bert_tokenizer(''.join(sentence),return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = self.bert_model(**bert_token_input).logits\n",
    "        \n",
    "        mask_token_index = (bert_token_input.input_ids == self.bert_tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "        predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "        predictions = self.bert_tokenizer.decode(predicted_token_id)\n",
    "\n",
    "        prediction_tokens = self.article.tokenize(predictions, keep_space=False)\n",
    "        prediction_tokens = self.remove_stopwords(prediction_tokens)\n",
    "        prediction_tokens = self.remove_history(prediction_tokens)\n",
    "        token_counter = Counter(prediction_tokens)\n",
    "        return token_counter.most_common(1)\n",
    "    \n",
    "    def best_guess(self, article):\n",
    "        guesses = []\n",
    "        sentences = self.article.sent_tokenize(article)\n",
    "        for sentence in sentences:\n",
    "            guesses.append(self.best_guess_sentence(sentence))\n",
    "        \n",
    "        sent_guess_dict = {}\n",
    "        for guess in guesses:\n",
    "            if len(guess) > 0:\n",
    "                try:\n",
    "                    sent_guess_dict[guess[0][0]] += guess[0][1]\n",
    "                except KeyError:\n",
    "                    sent_guess_dict[guess[0][0]] = guess[0][1]\n",
    "        \n",
    "        sorted_list = sorted(sent_guess_dict.items(), key=lambda x: x[1],reverse=True)\n",
    "        sorted_list = [x[0] for x in sorted_list]\n",
    "        return sorted_list\n",
    "    \n",
    "    def remove_stopwords(self,tokens):\n",
    "        token_list = []\n",
    "        for t in tokens:\n",
    "            if (not (t in self.article.revealed_dict)) and (len(t) > 1):\n",
    "                token_list.append(t)\n",
    "        return token_list\n",
    "\n",
    "    # Prevent making guesses that have already been guessed (found in history)\n",
    "    def remove_history(self,tokens):\n",
    "        token_list = []\n",
    "        for t in tokens:\n",
    "            if (not (t in self.history)):\n",
    "                token_list.append(t)\n",
    "        return token_list\n",
    "\n",
    "    def update_history(self,token):\n",
    "        self.history[token.casefold()] = None\n",
    "        self.history[token.capitalize()] = None\n",
    "    \n",
    "    def evaluateHitRatio():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n = 10\n",
    "run = 1\n",
    "hit_ratio = 0\n",
    "\n",
    "game = Game()\n",
    "game.setArticle(test_input)\n",
    "game.setY('architecture')\n",
    "\n",
    "PLAY = True\n",
    "while PLAY:\n",
    "    masked = ''.join(game.article.redact_article(bert_mask=True))\n",
    "    guesses =  game.best_guess(masked)\n",
    "    best_guess = guesses[0]\n",
    "    guesses = guesses[: n if len(guesses) > n else len(guesses)]\n",
    "    countHits = 0\n",
    "    for guess in guesses:\n",
    "        game.article.add_revealed(guess)\n",
    "        if guess == game.Y:\n",
    "            print('Redactle Word is: {}'.format(\n",
    "                guess\n",
    "            ))\n",
    "            PLAY = False\n",
    "        if guess in game.article.set_:\n",
    "            countHits += 1\n",
    "        if not PLAY: break\n",
    "    \n",
    "    if not PLAY: break\n",
    "    print('Run {}: {} hits.'.format(\n",
    "        run,\n",
    "        countHits\n",
    "    ))\n",
    "\n",
    "    hit_ratio += countHits / n\n",
    "    run += 1\n",
    "    if countHits == 0:\n",
    "        PLAY = False\n",
    "\n",
    "hit_ratio /= run\n",
    "print('Hit Ratio: {}, Rank: {}'.format(\n",
    "    hit_ratio,\n",
    "    run\n",
    "))\n",
    "print( ''.join(game.article.redact_article()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecture</td>\n",
       "      <td>Architecture (from Latin  architectura; from A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Art</td>\n",
       "      <td>Art is a diverse range of human activity, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artist</td>\n",
       "      <td>An artist is a person engaged in an activity r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Museum</td>\n",
       "      <td>A museum ( mew-ZEE-əm; plural museums or, rare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Literature</td>\n",
       "      <td>Literature is any collection of written work, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9938</th>\n",
       "      <td>Sailing</td>\n",
       "      <td>Sailing employs the wind—acting on sails, wing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9939</th>\n",
       "      <td>Sail</td>\n",
       "      <td>A sail is a tensile structure—made from fabric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9941</th>\n",
       "      <td>Shipbuilding</td>\n",
       "      <td>Shipbuilding is the construction of ships and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9942</th>\n",
       "      <td>Steamboat</td>\n",
       "      <td>A steamboat is a boat that is propelled primar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9948</th>\n",
       "      <td>Tractor</td>\n",
       "      <td>A tractor is an engineering vehicle specifical...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4434 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             title                                            content\n",
       "0     Architecture  Architecture (from Latin  architectura; from A...\n",
       "3              Art  Art is a diverse range of human activity, and ...\n",
       "4           Artist  An artist is a person engaged in an activity r...\n",
       "5           Museum  A museum ( mew-ZEE-əm; plural museums or, rare...\n",
       "9       Literature  Literature is any collection of written work, ...\n",
       "...            ...                                                ...\n",
       "9938       Sailing  Sailing employs the wind—acting on sails, wing...\n",
       "9939          Sail  A sail is a tensile structure—made from fabric...\n",
       "9941  Shipbuilding  Shipbuilding is the construction of ships and ...\n",
       "9942     Steamboat  A steamboat is a boat that is propelled primar...\n",
       "9948       Tractor  A tractor is an engineering vehicle specifical...\n",
       "\n",
       "[4434 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data/articles/'\n",
    "article_df = pd.read_csv(data_path + 'dataset.csv')\n",
    "article_df = article_df[['title','content']]\n",
    "article_df = article_df[~article_df['title'].str.contains(' ')]\n",
    "article_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>Milk</td>\n",
       "      <td>Milk is a white liquid food produced by the ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5121</th>\n",
       "      <td>Kilogram</td>\n",
       "      <td>The kilogram (also kilogramme) is the unit of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8112</th>\n",
       "      <td>Bursa</td>\n",
       "      <td>Bursa (Ancient Greek: Προῦσα, Latin: Prusa, Ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3805</th>\n",
       "      <td>Hemiptera</td>\n",
       "      <td>Hemiptera (; from Ancient Greek  hemipterus 'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>Coelacanth</td>\n",
       "      <td>The coelacanths ( (listen) SEE-lə-kanth) are f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8351</th>\n",
       "      <td>Kyrgyzstan</td>\n",
       "      <td>Kyrgyzstan, or the Kyrgyz Republic, is a landl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9859</th>\n",
       "      <td>Dye</td>\n",
       "      <td>A dye is a colored substance that chemically b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>Sense</td>\n",
       "      <td>A sense is a biological system used by an orga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4533</th>\n",
       "      <td>Turnip</td>\n",
       "      <td>The turnip or white turnip (Brassica rapa subs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5635</th>\n",
       "      <td>Bomb</td>\n",
       "      <td>A bomb is an explosive weapon that uses the ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7868</th>\n",
       "      <td>Benghazi</td>\n",
       "      <td>Benghazi () (lit. Son of [the] Ghazi) is a cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>Farmer</td>\n",
       "      <td>A farmer is a person engaged in agriculture, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4525</th>\n",
       "      <td>Onion</td>\n",
       "      <td>An onion (Allium cepa L., from Latin cepa mean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9878</th>\n",
       "      <td>Bicycle</td>\n",
       "      <td>A bicycle, also called a pedal cycle, bike or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>Sharia</td>\n",
       "      <td>Sharia (; Arabic: شريعة, romanized: sharīʿa [ʃ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9775</th>\n",
       "      <td>Clock</td>\n",
       "      <td>A clock or a timepiece is a device used to mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8886</th>\n",
       "      <td>Wildfire</td>\n",
       "      <td>A wildfire, forest fire, bushfire, wildland fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>Masturbation</td>\n",
       "      <td>Masturbation is the sexual stimulation of one'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>Pesticide</td>\n",
       "      <td>Pesticides are substances that are meant to co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7895</th>\n",
       "      <td>Houston</td>\n",
       "      <td>Houston ( (listen); HEW-stən) is the most popu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title                                            content\n",
       "748           Milk  Milk is a white liquid food produced by the ma...\n",
       "5121      Kilogram  The kilogram (also kilogramme) is the unit of ...\n",
       "8112         Bursa  Bursa (Ancient Greek: Προῦσα, Latin: Prusa, Ot...\n",
       "3805     Hemiptera  Hemiptera (; from Ancient Greek  hemipterus 'h...\n",
       "3870    Coelacanth  The coelacanths ( (listen) SEE-lə-kanth) are f...\n",
       "8351    Kyrgyzstan  Kyrgyzstan, or the Kyrgyz Republic, is a landl...\n",
       "9859           Dye  A dye is a colored substance that chemically b...\n",
       "3400         Sense  A sense is a biological system used by an orga...\n",
       "4533        Turnip  The turnip or white turnip (Brassica rapa subs...\n",
       "5635          Bomb  A bomb is an explosive weapon that uses the ex...\n",
       "7868      Benghazi  Benghazi () (lit. Son of [the] Ghazi) is a cit...\n",
       "2614        Farmer  A farmer is a person engaged in agriculture, r...\n",
       "4525         Onion  An onion (Allium cepa L., from Latin cepa mean...\n",
       "9878       Bicycle  A bicycle, also called a pedal cycle, bike or ...\n",
       "2260        Sharia  Sharia (; Arabic: شريعة, romanized: sharīʿa [ʃ...\n",
       "9775         Clock  A clock or a timepiece is a device used to mea...\n",
       "8886      Wildfire  A wildfire, forest fire, bushfire, wildland fi...\n",
       "874   Masturbation  Masturbation is the sexual stimulation of one'...\n",
       "5191     Pesticide  Pesticides are substances that are meant to co...\n",
       "7895       Houston  Houston ( (listen); HEW-stən) is the most popu..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(article_df, test_size=20)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Milk\n",
      "Run: 1, Hits: 8, Guesses: ['women', 'also', 'music', 'children', 'london', '2006', 'population', 'one', 'thes', 'two']\n",
      "Run: 2, Hits: 9, Guesses: ['used', 'non', 'number', 'english', 'others', 'love', 'people', 'john', '2011', 'country']\n",
      "Run: 3, Hits: 9, Guesses: ['found', 'food', 'first', 'man', 'smith', '10', '2007', 'source', 'UNK', 'based']\n",
      "Run: 4, Hits: 6, Guesses: ['common', 'old', 'history', 'however', 'work', 'died', 'use', 'things', 'laws', 'UNK']\n",
      "Run: 5, Hits: 8, Guesses: ['cases', 'language', 'system', 'UNK', 'mid', 'england', '50', 'song', 'temperature', 'water']\n",
      "Run: 6, Hits: 7, Guesses: ['UNK', 'uk', 'sun', 'official', 'include', 'according', 'high', 'among', 'better', 'like']\n",
      "Run: 7, Hits: 7, Guesses: ['UNK', 'world', 'city', 'later', 'although', 'ands', 'new', 'examples', 'could', 'white']\n",
      "Run: 8, Hits: 6, Guesses: ['UNK', 'australia', 'county', 'leader', 'word', 'york', 'time', 'taken', 'perhaps', '12']\n",
      "Run: 9, Hits: 7, Guesses: ['000', 'UNK', 'access', '2008', 'species', 'europe', 'black', 'life', 'main', 'school']\n",
      "Run: 10, Hits: 5, Guesses: ['UNK', 'story', 'good', 'want', 'fish', 'right', 'appeared', 'would', 'film', 'africa']\n",
      "Run: 11, Hits: 8, Guesses: ['UNK', 'south', 'back', 'able', 'something', 'said', 'king', '1990s', 'free', 'municipality']\n",
      "Run: 12, Hits: 6, Guesses: ['france', 'UNK', 'even', 'door', 'categories', 'way', 'player', '2010', 'types', 'human']\n",
      "Run: 13, Hits: 5, Guesses: ['UNK', 'data', 'university', 'end', 'important', 'alternative', 'family', 'case', 'seen', 'rules']\n",
      "Run: 14, Hits: 6, Guesses: ['UNK', 'languages', 'written', 'books', 'part', 'asia', 'schools', 'age', 'elsewhere', 'live']\n",
      "Run: 15, Hits: 6, Guesses: ['UNK', 'southeast', 'different', 'means', 'god', 'theory', 'early', 'thee', 'kind', 'apart']\n",
      "Run: 16, Hits: 8, Guesses: ['UNK', 'popular', 'defined', 'name', 'modern', 'states', 'possible', 'led', 'culture', 'india']\n",
      "Run: 17, Hits: 8, Guesses: ['UNK', 'largest', 'term', 'products', 'indian', 'well', 'canada', 'countries', 'become', 'united']\n",
      "Run: 18, Hits: 8, Guesses: ['UNK', 'producer', '2000', 'comes', 'get', 'made', 'consumer', 'definition', 'large', 'level']\n",
      "Run: 19, Hits: 5, Guesses: ['wood', 'UNK', 'industry', 'company', 'followed', 'rice', 'philippines', '100', 'refers', 'manufacturers']\n",
      "Run: 20, Hits: 4, Guesses: ['UNK', 'make', 'mean', 'presence', 'introduced', 'chinese', 'slaves', 'control', 'anywhere', 'changes']\n",
      "Word not found.\n",
      "Hit Ratio: 0.68, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Kilogram\n",
      "Run: 1, Hits: 6, Guesses: ['world', 'film', 'one', 'two', 'used', 'however', 'god', 'love', 'name', 'word']\n",
      "Run: 2, Hits: 9, Guesses: ['english', 'man', 'also', 'based', 'part', 'number', 'time', 'state', 'located', 'three']\n",
      "Run: 3, Hits: 4, Guesses: ['city', 'book', 'universe', 'called', 'point', 'police', 'would', 'space', 'women', 'system']\n",
      "Run: 4, Hits: 6, Guesses: ['new', 'note', 'use', 'way', 'UNK', 'law', 'location', 'words', 'example', 'reasons']\n",
      "Run: 5, Hits: 5, Guesses: ['things', 'still', 'UNK', 'parts', 'metric', 'ideas', 'sometimes', 'characters', 'object', 'finally']\n",
      "Run: 6, Hits: 6, Guesses: ['units', 'UNK', 'terms', 'according', 'theory', 'product', 'cases', 'defined', '45', 'committee']\n",
      "Run: 7, Hits: 6, Guesses: ['unit', 'army', 'single', 'length', 'derived', 'second', 'term', 'type', 'american', 'associated']\n",
      "Run: 8, Hits: 5, Guesses: ['first', 'team', 'UNK', 'following', 'per', 'points', 'land', 'definition', 'common', 'case']\n",
      "Run: 9, Hits: 7, Guesses: ['UNK', 'measurement', 'different', 'introduced', 'live', 'vehicles', 'proposed', 'since', 'half', 'measure']\n",
      "Run: 10, Hits: 4, Guesses: ['data', 'UNK', 'mathematical', 'church', 'mathematics', 'instrument', 'rover', 'work', 'sum', 'thus']\n",
      "Run: 11, Hits: 5, Guesses: ['UNK', 'end', 'foot', 'width', 'general', 'equipment', 'small', 'letter', 'london', 'therefore']\n",
      "Run: 12, Hits: 4, Guesses: ['function', 'UNK', 'map', 'originally', 'aircraft', 'table', 'day', 'period', 'buildings', 'people']\n",
      "Run: 13, Hits: 5, Guesses: ['UNK', 'military', 'equivalent', 'arrival', 'tower', 'technology', 'nature', 'rest', 'commonly', 'thing']\n",
      "Run: 14, Hits: 4, Guesses: ['UNK', 'natural', 'statistics', 'instead', 'physical', '1984', 'agrees', 'recognized', 'remained', 'types']\n",
      "Run: 15, Hits: 4, Guesses: ['UNK', 'include', 'modern', 'considered', 'objects', 'model', 'values', 'compass', 'home', 'properties']\n",
      "Run: 16, Hits: 6, Guesses: ['UNK', 'later', 'another', 'body', 'third', 'quantum', 'consensus', 'taking', 'written', 'equation']\n",
      "Run: 17, Hits: 5, Guesses: ['UNK', 'rule', 'states', 'corresponds', 'found', 'society', 'maximum', 'bess', 'consistent', 'degree']\n",
      "Run: 18, Hits: 3, Guesses: ['UNK', 'elements', '30', 'published', '2006', 'value', 'equal', 'continuous', 'source', 'school']\n",
      "Run: 19, Hits: 2, Guesses: ['UNK', 'absolute', '2s', 'element', 'starting', 'listed', 'temperature', 'founded', 'agreement', 'county']\n",
      "Run: 20, Hits: 3, Guesses: ['UNK', 'rules', 'earth', 'expressed', 'mass', 'due', '1969', 'territorial', 'village', 'murder']\n",
      "Word not found.\n",
      "Hit Ratio: 0.4950000000000001, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Bursa\n",
      "Run: 1, Hits: 5, Guesses: ['film', 'women', 'city', 'first', 'children', 'london', 'also', '2006', 'new', 'two']\n",
      "Run: 2, Hits: 7, Guesses: ['series', 'war', 'south', 'name', 'capital', 'music', 'world', 'UNK', 'god', 'important']\n",
      "Run: 3, Hits: 6, Guesses: ['english', 'north', '2011', 'UNK', 'one', 'books', 'thes', 'major', 'populous', 'house']\n",
      "Run: 4, Hits: 3, Guesses: ['UNK', 'time', 'second', 'jack', 'france', 'family', 'book', 'university', 'remains', 'part']\n",
      "Run: 5, Hits: 6, Guesses: ['district', 'UNK', 'period', 'known', 'however', 'students', 'founded', 'people', 'life', 'york']\n",
      "Run: 6, Hits: 5, Guesses: ['river', 'UNK', 'large', 'cities', 'military', 'england', 'ghana', 'apart', 'library', 'main']\n",
      "Run: 7, Hits: 6, Guesses: ['UNK', 'number', 'goa', 'bus', 'years', 'school', 'private', 'national', 'shortly', 'considered']\n",
      "Run: 8, Hits: 4, Guesses: ['UNK', 'man', 'work', 'east', 'buses', 'average', '1992', 'championship', '53', 'produced']\n",
      "Run: 9, Hits: 3, Guesses: ['UNK', 'macau', 'campus', 'album', 'water', 'see', 'rome', 'love', 'cup', 'third']\n",
      "Run: 10, Hits: 2, Guesses: ['UNK', 'nigeria', 'beautiful', 'order', 'death', 'canada', 'brought', 'american', 'united', 'referred']\n",
      "Run: 11, Hits: 4, Guesses: ['UNK', 'country', 'open', '1870', 'jerusalem', 'takes', 'son', 'majority', 'based', 'home']\n",
      "Run: 12, Hits: 3, Guesses: ['UNK', 'chicago', 'well', 'office', 'brazil', 'friends', 'located', 'others', 'preceded', 'left']\n",
      "Run: 13, Hits: 5, Guesses: ['UNK', 'election', 'largest', 'park', 'restaurants', 'high', 'artists', 'history', 'headquarters', 'lived']\n",
      "Run: 14, Hits: 3, Guesses: ['UNK', 'characters', 'small', 'political', 'schools', 'population', 'taken', 'philadelphia', 'museum', 'nearby']\n",
      "Run: 15, Hits: 5, Guesses: ['art', 'brooklyn', 'UNK', 'british', '000', 'economic', 'west', 'madrid', 'worked', 'proximity']\n",
      "Run: 16, Hits: 3, Guesses: ['UNK', 'social', 'growth', 'came', 'stadium', 'respectively', 'commercial', '1930s', 'sainta', 'help']\n",
      "Run: 17, Hits: 4, Guesses: ['UNK', 'industry', 'built', 'marx', 'capacity', 'rapid', 'duke', 'george', 'born', 'king']\n",
      "Run: 18, Hits: 6, Guesses: ['UNK', 'railway', 'many', 'manufacturing', 'designed', 'church', 'system', 'succeeded', 'john', 'ii']\n",
      "Run: 19, Hits: 4, Guesses: ['UNK', 'later', 'buildings', 'construction', 'metro', 'station', 'newness', 'downtown', 'athens', 'hungary']\n",
      "Run: 20, Hits: 3, Guesses: ['UNK', 'building', 'allegiance', 'peace', 'last', 'royal', 'america', 'post', 'jakarta', 'experienced']\n",
      "Word not found.\n",
      "Hit Ratio: 0.43500000000000005, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Hemiptera\n",
      "Run: 1, Hits: 5, Guesses: ['women', 'man', 'two', 'also', 'english', 'music', 'one', 'song', 'city', 'film']\n",
      "Run: 2, Hits: 6, Guesses: ['thes', 'people', 'new', 'found', 'used', 'family', 'back', 'put', 'trying', 'types']\n",
      "Run: 3, Hits: 7, Guesses: ['similar', 'species', 'room', 'worn', 'three', 'small', 'known', 'world', 'children', 'rare']\n",
      "Run: 4, Hits: 6, Guesses: ['birds', 'fish', 'large', 'many', 'half', 'went', 'based', 'dead', 'several', 'love']\n",
      "Run: 5, Hits: 4, Guesses: ['like', '2008', 'members', 'latin', 'full', 'term', 'words', 'matter', 'made', '000']\n",
      "Run: 6, Hits: 6, Guesses: ['number', 'duck', 'sometimes', 'others', 'different', 'refers', 'however', 'divided', 'around', 'national']\n",
      "Run: 7, Hits: 3, Guesses: ['lizard', 'nature', 'later', '30', 'use', 'society', 'university', 'games', 'things', 'better']\n",
      "Run: 8, Hits: 6, Guesses: ['australia', 'called', 'type', 'examples', 'appear', 'white', 'kind', 'sex', 'high', 'example']\n",
      "Run: 9, Hits: 4, Guesses: ['include', 'done', 'point', 'legs', 'color', 'temperatures', 'week', 'feeding', 'train', 'recent']\n",
      "Run: 10, Hits: 6, Guesses: ['human', 'water', 'animals', 'arms', 'long', 'never', 'along', 'first', 'clock', 'order']\n",
      "Run: 11, Hits: 2, Guesses: ['language', 'left', 'ferns', 'supply', 'head', 'king', 'tower', 'stone', '1900', 'common']\n",
      "Run: 12, Hits: 0, Guesses: ['system', 'eyes', 'UNK', 'lost', 'god', 'free', 'board', 'ability', '2011', 'iq']\n",
      "Word not found.\n",
      "Hit Ratio: 0.4583333333333333, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Coelacanth\n",
      "Run: 1, Hits: 5, Guesses: ['london', 'album', 'time', 'people', 'two', 'also', 'city', 'smith', 'davis', 'new']\n",
      "Run: 2, Hits: 6, Guesses: ['john', 'based', 'children', 'first', 'common', 'world', 'food', 'women', 'found', 'small']\n",
      "Run: 3, Hits: 6, Guesses: ['2006', 'story', 'species', 'important', 'china', 'together', 'non', 'thes', 'still', 'called']\n",
      "Run: 4, Hits: 6, Guesses: ['however', 'mediterranean', 'history', 'number', 'written', 'female', 'like', 'end', 'island', 'music']\n",
      "Run: 5, Hits: 7, Guesses: ['one', 'good', 'due', 'film', 'rate', 'period', 'well', 'song', 'would', 'kind']\n",
      "Run: 6, Hits: 3, Guesses: ['growing', 'source', 'men', 'said', 'UNK', 'coast', 'life', 'survive', 'australia', 'california']\n",
      "Run: 7, Hits: 6, Guesses: ['islands', 'west', 'others', 'known', 'half', 'although', 'UNK', 'greenland', 'project', '2012']\n",
      "Run: 8, Hits: 6, Guesses: ['100', 'humans', 'main', 'man', 'UNK', 'discovery', 'atlantic', 'replaced', 'door', 'member']\n",
      "Run: 9, Hits: 6, Guesses: ['character', 'population', 'believed', 'akin', 'african', 'UNK', 'animal', 'unique', 'different', 'game']\n",
      "Run: 10, Hits: 5, Guesses: ['southern', 'river', 'south', 'UNK', 'discover', 'continue', 'thing', 'band', 'short', 'plan']\n",
      "Run: 11, Hits: 3, Guesses: ['europe', 'government', 'UNK', 'early', 'shortly', 'nature', 'come', 'english', 'crete', 'connection']\n",
      "Run: 12, Hits: 3, Guesses: ['UNK', 'button', 'significant', 'reports', 'africa', 'respectively', 'cape', 'products', '50', 'likely']\n",
      "Run: 13, Hits: 4, Guesses: ['lizard', 'UNK', 'similar', 'may', 'wine', 'close', 'things', 'india', 'male', 'injury']\n",
      "Run: 14, Hits: 5, Guesses: ['going', 'group', 'UNK', 'pleistocene', 'function', '000', 'development', 'name', 'chorus', 'brown']\n",
      "Run: 15, Hits: 4, Guesses: ['UNK', 'system', 'work', 'since', '2008', '2010', 'three', 'america', 'best', 'gulf']\n",
      "Run: 16, Hits: 3, Guesses: ['part', 'closely', 'UNK', 'human', 'ball', 'roads', '2004', 'comes', 'types', 'frog']\n",
      "Run: 17, Hits: 6, Guesses: ['birds', 'UNK', 'related', 'entrances', 'includes', 'word', 'rare', 'fossil', 'british', 'living']\n",
      "Run: 18, Hits: 6, Guesses: ['mammals', 'water', 'fossils', 'fish', 'relationship', 'considered', 'greek', 'agriculture', 'review', 'restricted']\n",
      "Run: 19, Hits: 8, Guesses: ['family', 'reptiles', 'meaning', 'head', 'cretaceous', 'relative', 'large', 'high', 'falling', 'plant']\n",
      "Run: 20, Hits: 2, Guesses: ['dinosaurs', '2idae', 'lived', 'stone', 'eyes', 'age', 'came', 'general', 'death', 'relatives']\n",
      "Word not found.\n",
      "Hit Ratio: 0.5, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Kyrgyzstan\n",
      "Run: 1, Hits: 8, Guesses: ['music', 'also', 'women', 'new', 'city', 'first', 'however', 'people', 'said', 'film']\n",
      "Run: 2, Hits: 8, Guesses: ['one', 'number', 'band', 'name', 'children', 'men', '2006', 'book', 'team', 'time']\n",
      "Run: 3, Hits: 7, Guesses: ['game', 'based', 'english', 'two', 'man', 'canada', 'member', 'according', 'UNK', 'thes']\n",
      "Run: 4, Hits: 8, Guesses: ['life', 'used', 'society', 'UNK', 'games', 'girl', 'popular', 'end', 'ofs', 'day']\n",
      "Run: 5, Hits: 7, Guesses: ['america', 'system', 'big', 'world', 'would', 'UNK', 'every', 'full', 'order', 'smith']\n",
      "Run: 6, Hits: 8, Guesses: ['others', 'part', 'UNK', '30', 'plans', 'non', 'sunday', 'often', '50', 'food']\n",
      "Run: 7, Hits: 5, Guesses: ['UNK', 'album', 'year', 'made', 'river', 'brought', 'came', 'include', 'american', 'song']\n",
      "Run: 8, Hits: 3, Guesses: ['UNK', 'main', 'categories', 'major', 'rest', 'room', 'songs', 'way', 'series', 'fight']\n",
      "Run: 9, Hits: 7, Guesses: ['education', 'well', 'UNK', 'london', 'son', 'least', 'population', 'capital', 'europe', 'john']\n",
      "Run: 10, Hits: 5, Guesses: ['european', '000', 'common', 'france', 'following', 'UNK', 'pakistan', 'mississippi', 'earth', 'refers']\n",
      "Run: 11, Hits: 7, Guesses: ['country', 'members', 'second', 'academy', 'highest', 'UNK', '500', 'saint', 'anti', 'followed']\n",
      "Run: 12, Hits: 8, Guesses: ['party', '2000', 'economic', 'live', 'help', 'built', 'cities', 'UNK', 'third', '10']\n",
      "Run: 13, Hits: 7, Guesses: ['houses', 'development', 'economy', 'addition', 'australia', 'muslims', 'together', 'UNK', 'total', 'wine']\n",
      "Run: 14, Hits: 8, Guesses: ['majority', 'UNK', 'important', 'many', 'small', 'history', 'large', 'yes', 'school', 'national']\n",
      "Run: 15, Hits: 6, Guesses: ['UNK', 'students', 'different', 'anthem', 'high', 'tourism', 'muslim', 'buildings', 'republic', 'species']\n",
      "Run: 16, Hits: 7, Guesses: ['china', 'war', 'government', 'president', 'living', 'UNK', 'police', 'czech', 'christian', 'japan']\n",
      "Run: 17, Hits: 6, Guesses: ['india', 'russia', 'federal', 'obama', '2014', 'british', 'million', 'east', 'election', 'known']\n",
      "Run: 18, Hits: 8, Guesses: ['us', '2008', 'countries', 'north', 'held', 'decided', 'vice', 'william', 'area', 'UNK']\n",
      "Run: 19, Hits: 7, Guesses: ['largest', '2010', 'olympic', 'change', 'UNK', '80', '100', '2016', 'ukraine', 'numbers']\n",
      "Run: 20, Hits: 8, Guesses: ['best', 'schools', 'university', 'released', 'early', '20', 'UNK', 'international', 'bank', 'jewish']\n",
      "Word not found.\n",
      "Hit Ratio: 0.69, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Dye\n",
      "Run: 1, Hits: 5, Guesses: ['women', 'london', 'source', 'also', 'languages', 'people', 'time', 'city', 'music', 'john']\n",
      "Run: 2, Hits: 6, Guesses: ['smith', 'used', 'two', 'english', 'number', 'non', 'said', 'water', 'europe', 'part']\n",
      "Run: 3, Hits: 5, Guesses: ['food', 'university', 'son', 'based', 'derived', 'bicycles', 'data', 'process', 'come', 'species']\n",
      "Run: 4, Hits: 4, Guesses: ['words', 'man', 'films', 'may', 'working', 'first', 'described', 'however', 'single', 'letter']\n",
      "Run: 5, Hits: 4, Guesses: ['similar', 'often', 'end', 'made', '2011', 'families', 'substances', 'called', 'theses', 'carried']\n",
      "Run: 6, Hits: 3, Guesses: ['found', 'song', 'china', 'decades', 'near', 'common', 'door', 'specific', 'cars', 'events']\n",
      "Run: 7, Hits: 3, Guesses: ['person', 'word', 'thes', 'fossils', 'name', 'objects', 'control', 'value', 'determined', 'body']\n",
      "Run: 8, Hits: 4, Guesses: ['object', 'main', 'close', 'concept', 'UNK', 'proportion', 'degree', 'fast', 'dried', 'note']\n",
      "Run: 9, Hits: 5, Guesses: ['types', 'purpose', 'eaten', 'UNK', 'system', 'controlled', 'new', 'belong', 'usually', 'many']\n",
      "Run: 10, Hits: 5, Guesses: ['others', 'according', 'UNK', 'items', 'relates', 'open', 'compounds', 'angel', 'different', 'help']\n",
      "Run: 11, Hits: 5, Guesses: ['refers', 'make', 'museum', 'team', 'UNK', 'area', 'well', 'referred', 'kind', 'back']\n",
      "Run: 12, Hits: 3, Guesses: ['large', 'recently', 'UNK', 'extent', 'way', 'problem', 'quickly', 'box', 'home', 'able']\n",
      "Run: 13, Hits: 4, Guesses: ['room', 'UNK', 'indicate', 'could', 'turned', 'industries', 'flow', 'goods', 'field', 'believe']\n",
      "Run: 14, Hits: 2, Guesses: ['tobacco', 'history', 'showed', 'UNK', 'location', 'house', 'life', 'world', 'chicago', 'anywhere']\n",
      "Run: 15, Hits: 4, Guesses: ['ancient', 'trade', 'UNK', '2008', 'image', 'throughout', 'cotton', 'mid', 'california', 'successfully']\n",
      "Run: 16, Hits: 2, Guesses: ['things', 'use', 'UNK', 'version', 'kept', 'commodity', 'british', 'old', 'historically', 'standard']\n",
      "Run: 17, Hits: 1, Guesses: ['laser', 'tools', 'UNK', 'products', 'language', 'weapons', 'gold', 'ship', 'even', 'electricity']\n",
      "Run: 18, Hits: 4, Guesses: ['important', 'UNK', 'simple', 'magical', 'natural', 'dead', 'three', 'major', 'love', 'examples']\n",
      "Run: 19, Hits: 0, Guesses: ['elements', 'animals', 'given', 'UNK', 'ivory', 'military', '2014', 'album', 'film', 'refer']\n",
      "Word not found.\n",
      "Hit Ratio: 0.3631578947368421, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Sense\n",
      "Run: 1, Hits: 9, Guesses: ['women', 'water', 'people', 'one', 'two', 'also', 'however', 'others', 'person', 'time']\n",
      "Run: 2, Hits: 6, Guesses: ['world', 'number', 'food', 'god', 'london', 'music', 'song', 'children', 'way', 'system']\n",
      "Run: 3, Hits: 9, Guesses: ['point', 'used', 'see', 'things', 'english', 'use', 'example', 'different', 'process', 'points']\n",
      "Run: 4, Hits: 8, Guesses: ['control', 'concept', 'types', 'language', 'referred', 'film', 'common', 'main', 'difference', 'new']\n",
      "Run: 5, Hits: 8, Guesses: ['part', 'represented', 'refers', 'would', 'known', 'well', 'learning', 'systems', 'languages', 'work']\n",
      "Run: 6, Hits: 6, Guesses: ['name', 'word', 'words', 'games', 'information', 'ability', 'divided', 'communicate', 'found', 'thing']\n",
      "Run: 7, Hits: 7, Guesses: ['based', 'members', 'computer', 'city', 'data', 'source', 'type', 'player', 'nature', 'objects']\n",
      "Run: 8, Hits: 7, Guesses: ['first', 'theory', 'described', 'cars', 'history', 'true', 'room', 'construction', 'although', 'able']\n",
      "Run: 9, Hits: 3, Guesses: ['story', 'term', 'similar', 'numbers', 'australia', 'put', 'determine', 'windows', 'sense', 'classes']\n",
      "Redactle Title is: Sense\n",
      "Hit Ratio: 0.7, Rank: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Turnip\n",
      "Run: 1, Hits: 4, Guesses: ['one', 'white', 'used', 'people', 'population', 'english', 'water', 'two', 'number', 'city']\n",
      "Run: 2, Hits: 1, Guesses: ['women', 'black', 'however', 'plant', 'lines', 'line', 'list', 'thes', 'comes', 'move']\n",
      "Run: 3, Hits: 3, Guesses: ['known', 'person', 'cut', 'referred', 'total', 'minority', 'species', 'live', 'time', 'different']\n",
      "Run: 4, Hits: 2, Guesses: ['respectively', 'horse', 'small', 'combination', 'others', 'life', 'common', 'mentioned', 'adult', 'name']\n",
      "Run: 5, Hits: 5, Guesses: ['flowers', 'also', 'larger', 'like', 'type', 'identical', 'colour', 'tail', 'differ', 'characters']\n",
      "Run: 6, Hits: 4, Guesses: ['culture', 'smaller', 'plural', 'called', 'found', 'album', 'growing', 'letter', 'creamy', 'long']\n",
      "Run: 7, Hits: 4, Guesses: ['plants', 'fish', 'uk', 'leaf', 'use', 'names', 'edge', 'elsewhere', 'length', 'order']\n",
      "Run: 8, Hits: 2, Guesses: ['end', 'ones', 'non', 'based', 'types', 'little', 'shoes', 'effect', 'family', '2008']\n",
      "Run: 9, Hits: 6, Guesses: ['variation', 'part', 'ands', 'areas', 'cared', 'refers', 'food', 'reduced', 'say', 'grow']\n",
      "Run: 10, Hits: 5, Guesses: ['leaves', 'architecture', 'low', 'served', 'lower', 'feature', 'lakes', 'date', '10', 'function']\n",
      "Run: 11, Hits: 4, Guesses: ['hairs', 'attached', 'looks', 'green', 'protein', 'high', 'would', 'europe', 'typically', 'australia']\n",
      "Run: 12, Hits: 5, Guesses: ['asia', 'widely', 'term', 'hole', 'ground', 'preferred', 'similar', 'cm', 'algorithm', 'smalls']\n",
      "Run: 13, Hits: 4, Guesses: ['30', 'space', 'stone', 'tower', 'body', 'parts', 'weight', 'clover', 'size', 'visible']\n",
      "Run: 14, Hits: 2, Guesses: ['cities', 'females', 'veins', 'coloured', 'associated', 'upper', 'cutting', 'stems', 'machine', 'red']\n",
      "Run: 15, Hits: 3, Guesses: ['countries', 'color', 'head', 'mixing', 'smell', 'yellow', 'skin', 'amount', '50', 'year']\n",
      "Run: 16, Hits: 2, Guesses: ['star', 'dollar', 'surface', 'fresh', 'colored', 'invisible', 'ratio', '000', 'numbers', 'content']\n",
      "Run: 17, Hits: 7, Guesses: ['UNK', 'side', 'replacing', 'orange', 'ways', 'fat', 'large', 'per', 'foods', 'genus']\n",
      "Run: 18, Hits: 1, Guesses: ['birds', 'broad', 'football', 'brown', '2010', 'asl', 'percent', 'relatives', 'far', 'added']\n",
      "Run: 19, Hits: 1, Guesses: ['islands', '12', 'colors', 'nominate', 'many', 'dictionary', 'potatoes', 'stood', 'important', 'work']\n",
      "Run: 20, Hits: 4, Guesses: ['africa', 'well', 'mediterranean', 'caspian', 'hundreds', 'element', 'soon', 'provide', 'dish', 'built']\n",
      "Word not found.\n",
      "Hit Ratio: 0.34500000000000003, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Bomb\n",
      "Run: 1, Hits: 8, Guesses: ['object', 'children', 'also', 'however', 'two', 'world', 'time', 'new', 'one', 'water']\n",
      "Run: 2, Hits: 8, Guesses: ['used', 'war', 'based', 'non', 'people', 'important', 'system', 'english', 'china', 'history']\n",
      "Run: 3, Hits: 7, Guesses: ['women', 'commonly', 'death', 'common', 'type', 'number', 'british', '2006', 'removed', 'london']\n",
      "Run: 4, Hits: 7, Guesses: ['city', 'ii', 'refers', 'high', 'purposes', 'use', 'word', 'guns', 'island', 'car']\n",
      "Run: 5, Hits: 3, Guesses: ['instruments', 'term', 'language', 'small', 'school', 'self', 'transportation', 'students', 'adjective', 'team']\n",
      "Run: 6, Hits: 3, Guesses: ['specific', 'education', 'german', 'done', 'going', 'called', 'first', 'band', 'protagonist', 'attempts']\n",
      "Run: 7, Hits: 3, Guesses: ['refer', 'battle', 'game', 'part', 'life', 'book', 'case', 'europe', 'hospital', 'drugs']\n",
      "Run: 8, Hits: 6, Guesses: ['terms', 'occur', 'far', 'complex', 'games', 'order', 'injury', 'point', 'rate', 'types']\n",
      "Run: 9, Hits: 5, Guesses: ['person', 'view', 'different', 'depth', 'wall', 'caused', 'degree', 'must', 'written', 'street']\n",
      "Run: 10, Hits: 5, Guesses: ['generally', 'environment', 'words', 'grade', 'miles', 'iraq', 'air', 'river', 'especially', 'name']\n",
      "Run: 11, Hits: 5, Guesses: ['wind', 'modern', 'engine', 'field', 'difficult', 'aircraft', 'carried', 'individual', 'technology', 'featured']\n",
      "Run: 12, Hits: 7, Guesses: ['carrying', 'temperature', 'force', 'large', 'military', 'distance', 'speeds', 'accident', 'depending', 'procession']\n",
      "Run: 13, Hits: 4, Guesses: ['scale', 'may', 'effect', 'location', 'us', 'heat', 'knowledge', 'ins', 'royal', 'area']\n",
      "Run: 14, Hits: 5, Guesses: ['hot', 'work', 'take', 'produced', 'single', 'mounted', 'thes', 'left', 'film', 'restricted']\n",
      "Run: 15, Hits: 6, Guesses: ['energy', 'amount', 'university', 'sold', 'set', 'museum', 'forces', 'vessels', 'american', 'comes']\n",
      "Run: 16, Hits: 4, Guesses: ['latin', 'music', 'production', 'powerful', 'form', 'electromagnetic', 'navy', 'known', 'literature', 'similar']\n",
      "Run: 17, Hits: 4, Guesses: ['weapons', 'uses', 'warriors', 'army', 'gauge', 'songs', 'turn', 'tactics', 'alone', 'majority']\n",
      "Run: 18, Hits: 6, Guesses: ['fire', 'damage', 'gun', 'items', 'french', 'anti', 'weapon', 'heavy', 'electronic', 'classified']\n",
      "Run: 19, Hits: 5, Guesses: ['related', 'equipped', 'UNK', 'changes', 'create', 'nuclear', 'classification', 'cause', 'bomb', 'hammer']\n",
      "Redactle Title is: Bomb\n",
      "Hit Ratio: 0.531578947368421, Rank: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Benghazi\n",
      "Run: 1, Hits: 7, Guesses: ['film', 'world', 'war', 'new', 'also', 'man', 'women', 'one', 'friends', 'city']\n",
      "Run: 2, Hits: 8, Guesses: ['children', 'half', 'people', 'many', 'music', 'work', 'song', 'however', 'river', 'first']\n",
      "Run: 3, Hits: 9, Guesses: ['old', 'cities', 'british', 'name', 'similar', 'end', 'king', 'france', 'schools', '2011']\n",
      "Run: 4, Hits: 8, Guesses: ['london', 'kind', 'associated', 'buildings', 'popular', 'recent', 'day', 'back', 'number', 'came']\n",
      "Run: 5, Hits: 7, Guesses: ['books', 'years', 'restaurants', 'famous', 'architecture', 'later', 'common', '2006', 'room', 'captured']\n",
      "Run: 6, Hits: 7, Guesses: ['example', 'located', 'period', 'important', 'well', 'continued', 'book', 'capital', 'interest', 'two']\n",
      "Run: 7, Hits: 7, Guesses: ['education', 'pakistan', 'non', 'party', 'translated', 'less', 'building', 'private', 'large', 'water']\n",
      "Run: 8, Hits: 8, Guesses: ['based', 'school', 'ands', 'includes', 'largest', 'main', 'soldiers', 'development', 'major', 'increase']\n",
      "Run: 9, Hits: 9, Guesses: ['public', 'history', 'population', 'house', 'include', 'part', 'sports', 'divided', 'project', 'park']\n",
      "Run: 10, Hits: 8, Guesses: ['built', 'line', 'high', 'family', 'station', 'swimming', 'island', 'hell', 'heavily', 'cote']\n",
      "Run: 11, Hits: 6, Guesses: ['american', 'airport', 'involved', 'said', 'early', 'town', 'thei', 'bombed', 'attack', 'told']\n",
      "Run: 12, Hits: 8, Guesses: ['ii', 'ruins', '2000', 'including', 'rome', 'often', 'would', 'given', 'present', '20th']\n",
      "Run: 13, Hits: 9, Guesses: ['refers', 'called', 'small', 'used', 'little', 'left', 'construction', 'time', 'founded', 'became']\n",
      "Run: 14, Hits: 6, Guesses: ['latin', 'club', 'money', 'lived', 'state', 'official', 'jerusalem', 'great', 'human', 'housing']\n",
      "Run: 15, Hits: 4, Guesses: ['football', 'UNK', 'changed', 'rebuilt', '1990s', 'fact', 'foot', '1940', 'bomb', 'films']\n",
      "Run: 16, Hits: 4, Guesses: ['UNK', 'john', 'railway', 'began', 'due', 'everything', 'join', 'mid', 'broadway', 'class']\n",
      "Run: 17, Hits: 6, Guesses: ['UNK', 'change', '1980s', 'smith', 'country', 'germany', 'age', 'village', 'islands', 'player']\n",
      "Run: 18, Hits: 6, Guesses: ['UNK', 'port', '1960s', 'houses', 'romans', 'open', 'students', 'always', 'market', 'shakespeare']\n",
      "Run: 19, Hits: 4, Guesses: ['UNK', 'events', 'property', 'translation', 'central', 'england', 'university', 'governed', 'provided', '000']\n",
      "Run: 20, Hits: 7, Guesses: ['south', 'UNK', 'international', 'renamed', 'held', 'local', 'version', 'singapore', 'universities', 'library']\n",
      "Word not found.\n",
      "Hit Ratio: 0.6900000000000001, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Farmer\n",
      "Run: 1, Hits: 8, Guesses: ['person', 'one', 'women', 'music', 'city', 'usually', 'work', 'however', 'people', 'living']\n",
      "Run: 2, Hits: 3, Guesses: ['life', 'may', 'man', 'sports', 'words', 'children', 'interest', 'case', 'use', 'either']\n",
      "Run: 3, Hits: 5, Guesses: ['countries', 'commonly', 'according', 'teaching', 'williams', 'national', 'featured', 'award', 'used', 'solely']\n",
      "Run: 4, Hits: 5, Guesses: ['high', 'regulated', 'australia', 'london', 'refers', 'company', 'modern', 'owned', 'number', 'constitute']\n",
      "Run: 5, Hits: 2, Guesses: ['education', 'kind', 'day', 'manager', 'cases', 'office', 'foreign', '50', 'good', 'first']\n",
      "Run: 6, Hits: 3, Guesses: ['focused', 'land', 'single', 'persons', 'population', 'others', 'season', 'basis', 'took', 'david']\n",
      "Run: 7, Hits: 4, Guesses: ['property', 'known', 'two', 'term', 'farmer', 'possession', 'band', 'friends', 'jones', 'conversation']\n",
      "Redactle Title is: Farmer\n",
      "Hit Ratio: 0.42857142857142855, Rank: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Onion\n",
      "Run: 1, Hits: 8, Guesses: ['used', 'white', 'also', 'referred', 'english', 'children', 'women', 'new', 'one', 'world']\n",
      "Run: 2, Hits: 4, Guesses: ['black', 'europe', 'house', 'two', 'city', 'common', 'games', 'game', 'end', 'john']\n",
      "Run: 3, Hits: 7, Guesses: ['use', 'first', 'small', 'thes', 'water', 'name', 'came', 'storage', 'latin', 'system']\n",
      "Run: 4, Hits: 7, Guesses: ['book', 'stored', 'others', 'known', 'number', 'case', 'part', 'several', 'important', 'exception']\n",
      "Run: 5, Hits: 6, Guesses: ['often', 'non', 'described', 'well', 'battle', 'examples', 'color', 'thing', 'food', 'species']\n",
      "Run: 6, Hits: 5, Guesses: ['like', 'member', 'letters', '2008', 'made', 'top', 'methods', 'vol', 'eaten', 'history']\n",
      "Run: 7, Hits: 6, Guesses: ['cooked', 'fish', 'word', 'wood', 'countries', 'item', '2004', 'australia', 'translated', 'classified']\n",
      "Run: 8, Hits: 7, Guesses: ['red', 'due', 'low', 'genus', 'usually', 'star', 'group', 'dies', 'however', 'africa']\n",
      "Run: 9, Hits: 6, Guesses: ['military', 'average', 'music', 'found', 'orange', 'eggs', 'high', 'includes', 'song', 'meal']\n",
      "Run: 10, Hits: 5, Guesses: ['form', 'commonly', 'may', 'people', 'education', 'construction', 'language', 'term', 'would', 'money']\n",
      "Run: 11, Hits: 4, Guesses: ['derived', 'china', 'predators', 'rare', 'types', 'early', 'difficult', 'come', 'learning', 'agriculture']\n",
      "Run: 12, Hits: 5, Guesses: ['summer', 'bird', 'hunted', 'numbers', 'work', 'higher', '2006', 'least', 'many', 'smiled']\n",
      "Run: 13, Hits: 4, Guesses: ['time', 'general', 'larger', 'amount', 'stopped', 'swimming', 'level', 'line', 'notation', 'campaign']\n",
      "Run: 14, Hits: 5, Guesses: ['around', 'defined', 'slightly', 'males', 'interest', 'type', 'fall', 'plants', 'files', 'example']\n",
      "Run: 15, Hits: 4, Guesses: ['flowers', 'edible', 'whole', 'little', 'weight', 'left', 'expired', 'colors', 'body', 'back']\n",
      "Run: 16, Hits: 6, Guesses: ['ground', 'include', 'consists', 'spots', 'reference', 'see', 'device', 'india', 'home', 'list']\n",
      "Run: 17, Hits: 5, Guesses: ['native', 'wine', 'branches', 'followed', 'safe', 'differs', 'film', '12', 'grow', 'names']\n",
      "Run: 18, Hits: 3, Guesses: ['10', 'settlers', 'date', 'females', '2011', 'toad', 'thisia', 'bill', 'last', 'ofa']\n",
      "Run: 19, Hits: 6, Guesses: ['lizard', 'man', 'size', 'middle', 'separate', 'applied', 'words', 'joke', 'location', 'point']\n",
      "Run: 20, Hits: 1, Guesses: ['album', 'literature', 'pressure', 'lower', 'sister', 'ofia', 'survived', 'screen', 'practice', 'equipment']\n",
      "Word not found.\n",
      "Hit Ratio: 0.52, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Bicycle\n",
      "Run: 1, Hits: 8, Guesses: ['one', 'music', 'also', 'people', 'thes', 'women', 'person', 'however', 'new', 'common']\n",
      "Run: 2, Hits: 8, Guesses: ['two', 'children', 'blacks', 'line', 'man', 'used', 'type', 'water', 'time', 'others']\n",
      "Run: 3, Hits: 4, Guesses: ['english', 'books', 'men', 'types', 'letter', 'song', 'london', 'shaped', 'called', 'non']\n",
      "Run: 4, Hits: 6, Guesses: ['ball', 'game', 'house', 'later', 'species', 'ands', 'room', 'way', 'number', 'include']\n",
      "Run: 5, Hits: 8, Guesses: ['points', 'system', 'world', 'food', 'name', 'would', 'countries', 'work', 'important', 'half']\n",
      "Run: 6, Hits: 6, Guesses: ['point', 'students', 'cases', 'live', 'first', 'example', 'words', 'year', 'friends', 'died']\n",
      "Run: 7, Hits: 6, Guesses: ['born', 'refers', 'school', 'population', 'UNK', 'based', 'kind', 'similar', 'see', 'beer']\n",
      "Run: 8, Hits: 5, Guesses: ['may', 'UNK', 'space', 'city', 'side', 'since', 'four', 'examples', 'left', 'go']\n",
      "Run: 9, Hits: 6, Guesses: ['case', 'special', 'UNK', 'given', 'age', 'multi', 'included', 'author', 'car', 'main']\n",
      "Run: 10, Hits: 5, Guesses: ['games', 'UNK', 'single', 'theme', 'take', 'television', 'introduced', 'especially', 'variety', 'impact']\n",
      "Run: 11, Hits: 6, Guesses: ['uk', 'large', 'made', 'UNK', 'small', 'wall', 'use', 'word', 'love', 'commercial']\n",
      "Run: 12, Hits: 5, Guesses: ['toys', 'size', 'items', 'separate', 'key', 'order', 'parts', 'production', 'french', 'dictionary']\n",
      "Run: 13, Hits: 7, Guesses: ['wood', 'film', 'cars', 'series', 'make', 'acquired', 'UNK', 'company', 'known', 'associated']\n",
      "Run: 14, Hits: 5, Guesses: ['products', 'UNK', 'american', 'largest', 'ground', 'smith', 'second', 'songs', 'addition', 'scott']\n",
      "Run: 15, Hits: 7, Guesses: ['UNK', 'john', 'ways', 'university', 'novel', 'station', 'end', 'manufacturing', 'million', 'design']\n",
      "Run: 16, Hits: 6, Guesses: ['UNK', 'followed', 'appeared', 'body', '2006', 'changes', '50', 'among', 'past', 'class']\n",
      "Run: 17, Hits: 8, Guesses: ['double', 'UNK', 'animal', 'applications', 'three', 'moved', 'function', 'done', 'produced', 'us']\n",
      "Run: 18, Hits: 5, Guesses: ['computer', 'UNK', 'whether', 'manufacturers', 'renault', 'serie', 'construction', 'represented', 'problem', 'band']\n",
      "Run: 19, Hits: 5, Guesses: ['UNK', 'married', '10', 'mobile', 'classes', 'product', 'back', 'section', 'move', 'head']\n",
      "Run: 20, Hits: 7, Guesses: ['right', 'companies', 'UNK', '000', 'life', 'cost', 'disabilities', 'rest', 'often', 'could']\n",
      "Word not found.\n",
      "Hit Ratio: 0.615, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Sharia\n",
      "Run: 1, Hits: 7, Guesses: ['women', 'also', 'one', 'world', 'god', 'love', 'people', 'film', 'however', 'city']\n",
      "Run: 2, Hits: 7, Guesses: ['london', 'use', 'used', 'according', 'number', 'children', 'music', 'others', '2006', 'kind']\n",
      "Run: 3, Hits: 9, Guesses: ['person', 'new', 'first', 'men', 'would', 'two', 'non', 'house', 'thes', 'example']\n",
      "Run: 4, Hits: 9, Guesses: ['book', 'name', 'cases', 'based', 'life', 'woman', 'nature', 'work', '2008', 'system']\n",
      "Run: 5, Hits: 8, Guesses: ['case', 'time', 'may', 'song', 'classification', 'term', 'said', 'part', 'state', 'released']\n",
      "Run: 6, Hits: 9, Guesses: ['examples', 'refers', 'end', 'australia', 'death', 'still', 'found', 'important', 'man', 'self']\n",
      "Run: 7, Hits: 6, Guesses: ['put', 'including', '2011', 'friends', 'king', 'books', 'described', 'war', 'history', 'different']\n",
      "Run: 8, Hits: 10, Guesses: ['members', 'countries', 'types', 'concept', 'states', 'john', 'way', 'include', 'culture', 'language']\n",
      "Run: 9, Hits: 9, Guesses: ['germany', 'canada', 'english', 'along', 'relationship', 'modern', 'theory', 'political', 'level', 'france']\n",
      "Run: 10, Hits: 9, Guesses: ['society', 'law', 'word', 'governments', 'social', 'education', 'science', 'control', 'national', 'club']\n",
      "Run: 11, Hits: 8, Guesses: ['laws', 'rights', 'civil', 'legal', 'switzerland', 'common', 'concerned', 'scholars', 'smith', 'international']\n",
      "Run: 12, Hits: 9, Guesses: ['human', 'court', 'systems', 'marriage', 'right', 'england', 'published', 'politics', 'act', 'similar']\n",
      "Run: 13, Hits: 10, Guesses: ['many', 'four', 'european', 'made', 'related', 'make', 'age', 'belief', 'source', 'day']\n",
      "Run: 14, Hits: 8, Guesses: ['member', 'adopted', 'UNK', 'courts', 'divided', 'place', 'order', 'become', 'lack', 'known']\n",
      "Run: 15, Hits: 9, Guesses: ['rule', 'land', 'determine', 'university', 'judges', 'led', 'UNK', 'meaning', 'party', 'argue']\n",
      "Run: 16, Hits: 8, Guesses: ['UNK', 'words', 'british', 'early', 'married', 'see', 'constitution', 'times', 'often', 'allowed']\n",
      "Run: 17, Hits: 8, Guesses: ['UNK', 'report', 'practice', 'argues', 'differences', 'discrimination', 'developed', 'must', 'divorce', 'anti']\n",
      "Run: 18, Hits: 8, Guesses: ['UNK', 'western', 'majority', 'issues', 'criminal', 'due', 'penalty', '18', 'justice', 'well']\n",
      "Run: 19, Hits: 9, Guesses: ['muslim', 'status', 'evidence', 'protection', 'UNK', 'decisions', 'called', 'tend', 'followed', 'restrictions']\n",
      "Run: 20, Hits: 9, Guesses: ['islamic', 'islam', 'muslims', 'religion', 'established', 'UNK', 'religious', 'basis', 'decision', 'article']\n",
      "Word not found.\n",
      "Hit Ratio: 0.8450000000000001, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Clock\n",
      "Run: 1, Hits: 7, Guesses: ['london', 'world', 'also', 'one', 'two', 'game', 'man', 'album', 'UNK', 'number']\n",
      "Run: 2, Hits: 6, Guesses: ['time', 'player', 'UNK', 'new', 'city', '2006', 'people', 'first', 'film', 'may']\n",
      "Run: 3, Hits: 8, Guesses: ['however', 'system', 'house', 'UNK', 'clock', 'end', 'long', 'love', 'used', 'life']\n",
      "Redactle Title is: Clock\n",
      "Hit Ratio: 0.6999999999999998, Rank: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Wildfire\n",
      "Run: 1, Hits: 8, Guesses: ['music', 'also', 'world', 'children', 'people', 'water', 'used', 'london', 'time', 'however']\n",
      "Run: 2, Hits: 6, Guesses: ['use', 'food', 'others', 'women', 'one', 'thes', '2008', 'person', 'film', 'france']\n",
      "Run: 3, Hits: 7, Guesses: ['number', 'students', 'common', 'war', 'city', 'first', 'man', 'non', 'products', 'back']\n",
      "Run: 4, Hits: 6, Guesses: ['based', '30', '2011', '2006', 'live', 'due', 'would', 'self', 'home', 'end']\n",
      "Run: 5, Hits: 9, Guesses: ['50', 'may', 'made', 'weather', 'name', 'include', 'english', 'nature', 'lack', 'new']\n",
      "Run: 6, Hits: 5, Guesses: ['book', 'day', 'factors', 'work', 'company', 'kind', 'york', 'high', 'interest', 'cold']\n",
      "Run: 7, Hits: 7, Guesses: ['average', 'found', 'temperature', 'make', 'total', 'point', 'history', 'room', 'terms', 'level']\n",
      "Run: 8, Hits: 9, Guesses: ['types', 'population', 'many', 'changes', 'snow', 'knowledge', 'examples', 'earth', 'example', 'multi']\n",
      "Run: 9, Hits: 8, Guesses: ['well', 'effect', 'conditions', 'according', 'even', 'better', 'tech', 'school', 'said', 'hospital']\n",
      "Run: 10, Hits: 8, Guesses: ['location', 'things', 'data', 'including', 'police', 'factor', 'life', 'died', 'objects', 'mid']\n",
      "Run: 11, Hits: 8, Guesses: ['size', 'health', 'age', 'europe', 'china', 'space', 'areas', 'test', 'system', 'schools']\n",
      "Run: 12, Hits: 7, Guesses: ['urban', 'two', 'access', 'affected', 'information', 'safety', 'several', 'australia', 'order', 'free']\n",
      "Run: 13, Hits: 9, Guesses: ['important', 'country', 'low', 'status', 'quality', 'wind', 'events', 'especially', 'way', 'widespread']\n",
      "Run: 14, Hits: 8, Guesses: ['systems', 'good', 'climate', 'society', 'cities', 'half', 'increase', 'david', 'includes', 'known']\n",
      "Run: 15, Hits: 9, Guesses: ['control', 'change', 'uk', 'year', 'pollution', 'related', 'went', 'large', 'compared', 'decrease']\n",
      "Run: 16, Hits: 10, Guesses: ['numbers', '000', 'summer', 'distance', 'increased', 'part', '10', 'significant', 'research', 'lower']\n",
      "Run: 17, Hits: 9, Guesses: ['levels', 'species', 'recent', 'vary', 'countries', 'air', 'studies', 'occurs', 'john', 'period']\n",
      "Run: 18, Hits: 8, Guesses: ['risk', 'years', '2000', 'focus', 'standards', 'family', 'surface', 'led', 'caused', 'humidity']\n",
      "Run: 19, Hits: 9, Guesses: ['cancer', 'disease', 'exposure', '100', 'canada', 'area', 'reduce', '2012', 'house', 'much']\n",
      "Run: 20, Hits: 8, Guesses: ['street', 'fire', 'costs', 'elderly', 'depending', 'less', 'since', 'study', 'surrounded', 'protection']\n",
      "Word not found.\n",
      "Hit Ratio: 0.7900000000000001, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Masturbation\n",
      "Run: 1, Hits: 9, Guesses: ['one', 'women', 'also', 'man', 'london', 'said', 'park', 'name', 'people', 'others']\n",
      "Run: 2, Hits: 8, Guesses: ['song', 'used', 'went', 'time', 'would', 'side', 'history', 'music', 'world', 'intro']\n",
      "Run: 3, Hits: 3, Guesses: ['understand', 'food', 'new', 'queen', 'smith', 'family', 'words', 'wall', 'player', 'children']\n",
      "Run: 4, Hits: 6, Guesses: ['john', 'may', 'however', 'thes', '50', '20', 'according', 'little', 'street', '2006']\n",
      "Run: 5, Hits: 10, Guesses: ['men', 'first', 'part', '2008', 'book', 'love', 'city', 'live', 'period', 'concept']\n",
      "Run: 6, Hits: 8, Guesses: ['day', 'found', 'death', 'religion', 'film', 'sitting', 'since', 'songs', 'born', 'object']\n",
      "Run: 7, Hits: 7, Guesses: ['road', 'dead', 'past', 'half', 'use', 'common', 'child', 'go', 'woman', 'end']\n",
      "Run: 8, Hits: 9, Guesses: ['two', 'hands', 'many', 'knowledge', 'study', 'example', 'letter', 'members', 'english', 'written']\n",
      "Run: 9, Hits: 8, Guesses: ['feet', 'hand', 'based', 'beautiful', 'word', 'associated', 'university', 'disease', 'cases', 'wrote']\n",
      "Run: 10, Hits: 9, Guesses: ['hold', 'god', 'old', 'back', 'sometimes', 'known', 'yes', 'individual', 'famous', '2011']\n",
      "Run: 11, Hits: 7, Guesses: ['lying', 'power', 'self', 'tools', 'work', 'health', 'york', 'wire', 'europe', 'include']\n",
      "Run: 12, Hits: 8, Guesses: ['examples', 'term', 'means', 'terms', 'head', 'life', 'right', 'green', 'sent', 'relationship']\n",
      "Run: 13, Hits: 6, Guesses: ['person', 'UNK', 'tend', 'education', 'johnson', 'specific', 'changed', 'see', 'although', 'determination']\n",
      "Run: 14, Hits: 6, Guesses: ['UNK', 'information', 'early', 'species', 'walking', 'wait', 'put', 'bed', 'purposes', 'fire']\n",
      "Run: 15, Hits: 4, Guesses: ['books', 'UNK', 'often', 'room', 'house', 'names', 'childhood', 'ready', 'depending', 'high']\n",
      "Run: 16, Hits: 5, Guesses: ['location', 'body', 'france', 'purpose', 'sleep', 'usually', 'idea', 'problem', 'game', 'looking']\n",
      "Run: 17, Hits: 7, Guesses: ['parts', 'blood', 'personal', 'possible', 'year', 'future', 'telephone', 'pillow', 'map', 'damage']\n",
      "Run: 18, Hits: 5, Guesses: ['sex', 'using', 'way', 'social', 'places', 'mouth', 'computer', 'face', 'event', 'mode']\n",
      "Run: 19, Hits: 6, Guesses: ['age', 'studies', 'hair', 'number', '2013', 'description', 'order', 'party', 'killed', 'happening']\n",
      "Run: 20, Hits: 7, Guesses: ['home', 'recent', 'years', 'sexual', 'communicate', 'large', 'away', 'presence', 'collection', 'working']\n",
      "Word not found.\n",
      "Hit Ratio: 0.69, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Pesticide\n",
      "Run: 1, Hits: 8, Guesses: ['also', 'use', 'water', 'women', 'people', 'world', 'music', 'non', 'history', 'children']\n",
      "Run: 2, Hits: 9, Guesses: ['london', 'based', 'used', 'part', 'good', 'number', 'two', 'person', 'war', 'europe']\n",
      "Run: 3, Hits: 8, Guesses: ['one', 'persons', 'time', 'others', 'common', 'things', 'name', 'would', 'followed', '1990s']\n",
      "Run: 4, Hits: 8, Guesses: ['example', 'released', 'uk', 'often', '2008', 'animals', 'word', 'image', 'science', 'include']\n",
      "Run: 5, Hits: 5, Guesses: ['examples', 'clock', 'food', 'said', 'describe', 'thes', 'defined', 'systems', 'humans', '2006']\n",
      "Run: 6, Hits: 6, Guesses: ['means', 'transportation', 'system', 'live', 'us', 'weapons', 'first', 'city', 'species', 'objects']\n",
      "Run: 7, Hits: 6, Guesses: ['fish', 'animal', 'known', 'life', 'similar', 'english', '100', 'point', 'kind', 'location']\n",
      "Run: 8, Hits: 6, Guesses: ['found', 'case', 'man', 'low', 'plants', 'object', 'royal', 'difficult', 'horse', 'tool']\n",
      "Run: 9, Hits: 4, Guesses: ['plant', 'consumed', 'something', '2011', 'study', 'lack', 'term', 'specials', 'special', 'translated']\n",
      "Run: 10, Hits: 6, Guesses: ['seed', 'technology', 'every', 'according', 'cost', 'ornamental', 'hybrids', 'cases', 'carried', 'produced']\n",
      "Run: 11, Hits: 3, Guesses: ['film', 'root', 'introduced', 'computer', 'well', 'published', 'yes', 'caused', 'pulled', 'england']\n",
      "Run: 12, Hits: 6, Guesses: ['small', 'release', 'particular', 'hand', 'new', 'building', 'derived', 'units', 'lot', 'made']\n",
      "Run: 13, Hits: 6, Guesses: ['removed', 'changes', 'work', 'compounds', 'beginning', 'seeds', 'words', 'end', 'modern', 'functions']\n",
      "Run: 14, Hits: 5, Guesses: ['free', 'anti', 'day', 'source', 'identified', 'see', 'harvested', 'following', 'diseases', 'cooking']\n",
      "Run: 15, Hits: 9, Guesses: ['may', 'make', 'protection', 'tend', 'countries', '2000', '2007', 'majority', 'parts', 'compared']\n",
      "Run: 16, Hits: 8, Guesses: ['act', 'rules', 'national', 'refers', 'health', 'broken', 'australia', 'begun', '000', 'another']\n",
      "Run: 17, Hits: 5, Guesses: ['safety', 'add', 'led', 'level', 'important', 'since', 'acres', 'terms', 'however', 'need']\n",
      "Run: 18, Hits: 6, Guesses: ['safe', 'definition', 'several', 'different', 'standards', 'factors', 'mental', 'computers', 'contact', 'child']\n",
      "Run: 19, Hits: 6, Guesses: ['dangerous', 'related', 'environment', 'death', 'cause', 'france', 'say', 'organization', 'present', 'ands']\n",
      "Run: 20, Hits: 8, Guesses: ['disease', 'harm', 'nature', 'birds', 'boil', 'bands', 'deaths', 'risk', 'scale', 'chemistry']\n",
      "Word not found.\n",
      "Hit Ratio: 0.6399999999999999, Rank: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Houston\n",
      "Run: 1, Hits: 9, Guesses: ['music', 'film', 'two', 'women', 'also', 'man', 'one', 'world', 'however', 'new']\n",
      "Run: 2, Hits: 6, Guesses: ['first', 'city', 'thes', 'album', '2011', 'children', 'based', 'war', 'london', 'song']\n",
      "Run: 3, Hits: 8, Guesses: ['people', 'population', 'others', '2008', 'band', 'street', 'white', 'chicago', 'love', 'time']\n",
      "Run: 4, Hits: 10, Guesses: ['said', '10', '100', '2014', 'according', '500', 'member', 'would', 'number', 'populous']\n",
      "Run: 5, Hits: 6, Guesses: ['000', 'best', 'non', 'district', 'capital', 'home', 'japan', 'name', 'back', 'door']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     masked \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(game\u001b[38;5;241m.\u001b[39marticle\u001b[38;5;241m.\u001b[39mredact_article(bert_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m---> 18\u001b[0m     guesses \u001b[38;5;241m=\u001b[39m  \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_guess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     guesses \u001b[38;5;241m=\u001b[39m guesses[: n \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(guesses) \u001b[38;5;241m>\u001b[39m n \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(guesses)]\n\u001b[1;32m     20\u001b[0m     countHits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn [4], line 39\u001b[0m, in \u001b[0;36mGame.best_guess\u001b[0;34m(self, article)\u001b[0m\n\u001b[1;32m     37\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marticle\u001b[38;5;241m.\u001b[39msent_tokenize(article)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m---> 39\u001b[0m     guesses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_guess_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     41\u001b[0m sent_guess_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m guess \u001b[38;5;129;01min\u001b[39;00m guesses:\n",
      "Cell \u001b[0;32mIn [4], line 23\u001b[0m, in \u001b[0;36mGame.best_guess_sentence\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     21\u001b[0m bert_token_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sentence),return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbert_token_input\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     25\u001b[0m mask_token_index \u001b[38;5;241m=\u001b[39m (bert_token_input\u001b[38;5;241m.\u001b[39minput_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_tokenizer\u001b[38;5;241m.\u001b[39mmask_token_id)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnonzero(as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m predicted_token_id \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m, mask_token_index]\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1347\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[39m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1341\u001b[0m \u001b[39m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m \u001b[39m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1347\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1348\u001b[0m     input_ids,\n\u001b[1;32m   1349\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1350\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1351\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1352\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1353\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1354\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1355\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1356\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1357\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1358\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1359\u001b[0m )\n\u001b[1;32m   1361\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1362\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1014\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1005\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1007\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1008\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1009\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1015\u001b[0m     embedding_output,\n\u001b[1;32m   1016\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1017\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1018\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1019\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1020\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1021\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1022\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1023\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1024\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1025\u001b[0m )\n\u001b[1;32m   1026\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1027\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:603\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    594\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    595\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    596\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    602\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    604\u001b[0m         hidden_states,\n\u001b[1;32m    605\u001b[0m         attention_mask,\n\u001b[1;32m    606\u001b[0m         layer_head_mask,\n\u001b[1;32m    607\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    609\u001b[0m         past_key_value,\n\u001b[1;32m    610\u001b[0m         output_attentions,\n\u001b[1;32m    611\u001b[0m     )\n\u001b[1;32m    613\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    614\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:531\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    528\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    529\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 531\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    532\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    533\u001b[0m )\n\u001b[1;32m    534\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    536\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pytorch_utils.py:246\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 246\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:543\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m--> 543\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[1;32m    544\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    545\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:443\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 443\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    444\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    445\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "mean_hit_ratio = 0\n",
    "mean_relative_ranking = 0\n",
    "class EndGame(Exception): pass\n",
    "for index, row in test.iterrows():\n",
    "    run = 0\n",
    "    hit_ratio = 0\n",
    "    game = Game()\n",
    "    game.setArticle(row.content)\n",
    "    game.setY(row.title)\n",
    "    print('Title: {}'.format(\n",
    "        game.Y\n",
    "    ))\n",
    "    found = False\n",
    "    try:\n",
    "        while True:\n",
    "            masked = ''.join(game.article.redact_article(bert_mask=True))\n",
    "            guesses =  game.best_guess(masked)\n",
    "            guesses = guesses[: n if len(guesses) > n else len(guesses)]\n",
    "            countHits = 0\n",
    "\n",
    "            for guess in guesses:\n",
    "                game.article.add_revealed(guess)\n",
    "\n",
    "                if guess in game.article.set_:\n",
    "                    countHits += 1\n",
    "            \n",
    "            print('Run: {}, Hits: {}, Guesses: {}'.format(\n",
    "                run+1,\n",
    "                countHits,\n",
    "                guesses\n",
    "            ))\n",
    "            hit_ratio += countHits / n\n",
    "            run += 1\n",
    "            if game.Yset.issubset(game.article.revealed_dict):\n",
    "                found = True\n",
    "                raise EndGame\n",
    "            if countHits == 0 or run == 20:\n",
    "                raise EndGame\n",
    "    except EndGame:\n",
    "        hit_ratio /= run\n",
    "        if found:\n",
    "            print('Redactle Title is: {}'.format(\n",
    "                game.Y\n",
    "            ))\n",
    "        else:\n",
    "            run = 20\n",
    "            print('Word not found.')\n",
    "        \n",
    "        mean_hit_ratio += hit_ratio\n",
    "        mean_relative_ranking += run\n",
    "        print('Hit Ratio: {}, Rank: {}'.format(\n",
    "            hit_ratio,\n",
    "            run\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHR: 0.5558320802005013, MRR: 16.9\n"
     ]
    }
   ],
   "source": [
    "mean_hit_ratio /= len(test)\n",
    "mean_relative_ranking /= len(test)\n",
    "print('MHR: {}, MRR: {}'.format(\n",
    "    mean_hit_ratio,\n",
    "    mean_relative_ranking\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fcb716559e0925484a16f1bb1776a6305c06a4355bc445a9bb3688c30f1d2e03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
